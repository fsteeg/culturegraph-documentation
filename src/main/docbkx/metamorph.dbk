<?xml version="1.0" encoding="UTF-8"?>
<chapter xmlns="http://docbook.org/ns/docbook" xmlns:xlink="http://www.w3.org/1999/xlink"
	xmlns:xml="http://www.w3.org/XML/1998/namespace" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
	xsi:schemaLocation="http://docbook.org/ns/docbook http://docbook.org/ns/docbook ">

	<title>Metamorph</title>

	<section>

		<title>Introduction</title>

		<para>
			Metamorph is a Java library including an XML based data
			transformation
			language, designed to ease dealing with metadata. In
			this document
			the
			design rationale behind Metamorph is discussed,
			followed by a user
			guide including many hands-on code examples. As a
			conclusion several
			real-world application scenarios are presented.
		</para>
		<para>
			Metadata is a central ingredient of any information storage and
			retrieval
			system. Being defined as ``data on data'', metadata provides
			descriptive information on the items stocked in the system; be it a
			library, an archive, a museum or a web search engine.
		</para>
		<para>
			The variety of possibly stored items is reflected in the
			plethora of
			existing metadata formats. Obviously, a museum needs to
			describe its
			exposition items differently than a library its available
			books. And
			even the same book may be described differently depending
			on the
			institution holding it: A library has different description
			needs
			than
			a book seller. The first might use a standard such as Marc
			21, the
			latter might use ONIX. Furthermore, standards also differ
			depending
			on
			the country or the language. Finally new technologies
			bring up new
			information needs not reflected in existing standards.
			%New standards addressing the needs of the electronic age include for
			instance Dublin Core, METS and MODS.
		</para>
		<para>
			In the face of a babylonic confusion of tongues,
			interoperability is an
			important issue, and metadata is transformed
			from one format to
			another on a regular basis. Transformations are not
			only performed to
			exchange data between institutions or to enable
			cross-collection
			searches. Metadata also needs to be transformed for
			indexing or for
			presentation in user-interfaces (on a webpage as HTML,
			for example).
			Despite being such an ubiquitous task, metadata
			transformation is
			still a tedious, mostly manual task, with scarce
			tool support.
			Software written to perform transformations is often
			coded from
			scratch for each and every individual case; despite great
			potential
			for component reuse.
		</para>
		<para>
			The purpose of this document is to explain how the metadata
			challenge is
			addressed by Metamorph, a Java library including a domain
			specific
			metadata transformation language expressed in XML. In the
			next
			section
			the classical metadata transformation process will be
			sketched to
			elucidate the inherent challenges and to motivate the
			requirements
			and
			design goals of Metamorph. Next, the general
			architecture will be
			presented, followed by a description of the
			language features.
			Metamorph was applied in various projects at the
			German National
			Library. In section \ref{apps} these application
			scenarios will be
			discussed.
			A final section on the future prospects
			and limitations of Metamorph
			rounds up this document.
		</para>

	</section>



	<section>
		<title>Metadata Transformation in Practice</title>
		<para>
			Metadata transformation is
			a complex and sometimes tedious
			process as its
			correctness depends on an
			overlap of domain knowledge
			and programming
			skills. First the typical
			procedure will be sketched
			followed by a
			discussion of the resulting
			challenges.
		</para>

		<para>
			\subsection{Workflow}

			The first step is to create a Crosswalk
			or
			Conceptual Mapping between the
			two formats. Such a Conceptual Mapping
			normally consists of a table in
			human readable form and it is created
			and maintained by domain experts.
			Figure \ref{fig:mapping_simple} shows
			two simple mapping rules from
			Pica to RDF, taken from the DNB Linked
			Data Service.
		</para>
		<para>
			\begin{figure}[htp]
			\centering
			\includegraphics[width=.8\textwidth]{figures/mapping_simple}
			\caption{Conceptual mapping for the DNB Linked Data
			Service}\label{fig:mapping_simple}
			\end{figure}

			On the left we see the
			respective fields in Pica, followed by an
			explanation of the content.
			Next, the target in RDF is defined,
			followed by remarks, indicating
			transformations applied to the data.
			The first line thus reads: Given
			a
			Pica+ recode, take the value of
			field {\tt 007P} subfield {\tt 0},
			prepend {\tt (DE-588b)} and write it
			to the RDF element {\tt
			rdaGr2:identifierForTheCorporateBody}. Figure
			\ref{fig:mapping_complex}
			shows a slightly more complex example. In
			this case values of different
			subfields need to be combined to fill
			the
			RDF element {\tt
			gnd:preferredNameForTheCorporateBody}. The last
			column
			defines this
			combination by reference to the subfield names.
		</para>
		<para>
			\begin{figure}[htp]
			\centering
			\includegraphics[width=.8\textwidth]{figures/mapping_complex}
			\caption{A more complex mapping for the DNB Linked Data
			Service}\label{fig:mapping_complex}
			\end{figure}



			Based on the
			conceptual
			mapping, a piece of software is developed which
			implements
			the mapping.
			This is done by a programmer based the
			conceptual mapping
			document.
		</para>



		{Challenges}

		The just described workflow poses several
		challenges which shall now be
		briefly discussed. In section \ref{arch}
		I
		will refer to them while
		introducing the architecture of Metamorph.

		\subsubsection{From Concept to Code}

		The transition forms the
		conceptual
		mapping to the actual code
		implementing it is a critical one.
		A
		conceptual mapping leaves space
		for interpretation. The programmer is
		the one to fill this gap,
		although he or she lacks the knowledge of the
		domain specialist.
		Information flows one-way from domain specialist to
		programmer. What
		the programmer really implements cannot be
		double-checked by the domain
		specialist as he or she in turn lacks the
		fluency in the respective
		computer language the transformation is
		realized in.


		\subsubsection{Format Independence}\label{format_ind}

		Conceptual mapping between metadata schemata bear a lot of
		resemblance.
		The table
		structure in which the mapping is described is
		almost always
		the same.
		The software side implementing the mappings
		differs
		significantly,
		though. This is due to encoding details of the
		metadata
		formats. For
		instance the code and data structures or classes
		used load
		and
		represent Marc 21 records and pica records in a Java
		program differ,
		even though conceptually they are very similar. Both
		are composed of
		fields containing subfields. Slight differences in the
		implementations
		renders reuse of the code infeasible.


		\subsubsection{Performance}

		The
		input to a transformation from one
		format to another may well comprise
		millions of records. Performance is
		thus an issue. It is also
		important
		to note that transformations have to
		be performed repeatedly as the
		metadata keeps changing or errors in the
		mapping are discovered.

		\section{Architecture}\label{arch}

		This section
		introduces the general
		architecture of Metamorph, pointing out
		how it
		addresses the
		requirements of reusability, transparency and
		performance.

		\subsection{Generic Data Structure}

		At least in the library
		domain, the
		organization of the data within a
		record is fairly similar
		across
		formats (Marc, Pica and Mab2, for
		instance): A record consists of
		named
		fields, which in turn contain
		named subfields. The actual data is
		stored
		in these subfields. Many
		formats also store meta-information such
		as
		modification date in the
		record\footnote{Which would actually be
		meta-meta-data, to be
		precise.}. Metadata formats in the archive domain
		in turn tend to
		exhibit more hierarchical structures. As pointed out in
		section
		\ref{format_ind}, a common denominator is needed to enable code
		reuse.
		Metamorph thus makes the assumption that the structure depicted
		in
		figure \ref{fig:record2_class} may serve as such a common
		denominator.
		The structure is fairly general and allows also for
		hierarchical
		structures. It turns out that most data structures found
		in
		common
		metadata formats can be mapped to it easily.

		In general {\tt
		Record} forms a self contained and independent unit. It
		may contain
		Literals or Entities which themselves may contain further
		Entities or
		Literals. A Field in library metadata (e.g. Pica) would map
		to an
		Entity, Subfields to Literals. The recursive organization of
		Entities
		whould not be called on in this case.
		\begin{figure}[htp]
		\centering
		\includegraphics[width=.90\textwidth]{figures/record2_class}
		\caption{The abstract data model underlying
		Metamorph}\label{fig:record2_class}
		\end{figure}

		The universality of
		data
		structure described in the previous section comes
		at a cost. To
		generate
		it explicitly is costly as it comprises many
		Java object such
		as
		literals, lists or maps, depending on the specific
		implementation.
		Handling such generic data structures normally results
		in convoluted
		if-infested code.
		The solution to this dilemma is twofold: Firstly, as
		described in the next
		subsection we avoid ever explicitly instantiating
		the data structure in
		figure \ref{fig:record2_class}. Secondly, we use
		a
		domain specific
		language to handle transformation on the data stream,
		as
		sketched in
		subsection \ref{trans}.

		\subsection{Data as a Stream of
		Events}

		Do we really need to explicitly construct the data structure
		for
		each
		record? No, it turns out that it can be easily serialized to a
		stream
		of events. Listing \ref{streamreceiver} shows the interface, a
		receiver
		of such a stream needs to implement.
		\begin{lstlisting}[float=htb, label=streamreceiver,caption=The
		interface used to serialize the data
		structure in figure
		\ref{fig:record2_class}., language=Java]
		public interface
		StreamReceiver
		{
		void startRecord(String identifier);
		void
		endRecord();
		void
		startEntity(String name);
		void endEntity();
		void
		literal(String name,
		String value);
		}
		\end{lstlisting}
		Adding an interface such as the one in
		listing \ref{streamsender} we can
		easily build processing chains using
		independent modules.

		\begin{lstlisting}[float=htb,
		label=streamsender,caption={The interface
		of an event stream sender. By
		virtue of the template R we can
		efficiently define processing chains
		via
		method chaining. See listing
		\ref{codepipeline2} for examples.},
		<programlisting language="java">
		public interface StreamSender {
		&lt;R extends StreamReceiver&gt;
			R setReceiver(R streamReceiver);
			}
		</programlisting>

		One
		module
		implementing {\tt StreamSender} for example might read a
		bibliographic
		record encoded in Pica and translating it to events.
		The
		events are
		received by another module which implements {\tt
		StreamReceiver} and
		{\tt StreamSender}, thus forming a pipe or filter
		element in the chain.
		It may react on the events and transform the
		stream and its contents.
		Finally an arbitrary {\tt StreamReceiver} may
		form the endpoint of
		chain by reassembling the events to objects or
		persisting them to a
		database, logging them, indexing them. The
		interfaces shield the
		intricacies of one element in the chain from the
		others. This means
		that parts can be easily exchanged. The modularity
		gained by this
		schema enables reuse of software components and admits
		for a more
		flexible architecture.

		Having modeled complex data as a
		stream of
		events, we can now build generic
		operation to be applied to
		this
		stream. Handling the stream and
		transforming it is the purpose of
		Metamorph which will be explained in
		the following section.

	</section>

	<section>
		<title>Using Metamorph</title>

		Figure \ref{fig:readerwriter} shows the basic
		setup for data processing with
		Metamorph. The processing pipeline
		starts with the input data which is
		read by a reader specific to the
		input format. The reader emits
		messages according to the {\tt
		StreamReceiver} interface (listing
		\ref{streamreceiver}). Due to this
		interface we are free to plug into
		the processing pipeline a variety of
		building blocks. In the majority
		of use cases the data needs to be
		transformed in one way or the other,
		a task that falls to the Metamorph
		object, depicted in the center of
		figure \ref{fig:readerwriter}. The
		actual transformation performed by
		Metamorph is encoded in the
		Metamorph definition file. Finally, a
		writer condenses the event stream
		into the target data format.

		\begin{figure}[htp]
		\centering
		\includegraphics[width=.70\textwidth]{figures/readerwriter}
		\caption{A
		typical processing pipeline including a Metamorph object for data
		transformation.}\label{fig:readerwriter}
		\end{figure}


		This section
		explains the how such a setup is implemented in practice.
		Subsection
		\ref{java} is dedicated to the Java code needed to wire the
		pipeline
		parts together, subsection \ref{def_lang} focuses on the
		Metamorph
		definition language.


		\subsection{The Java Side}\label{java}

		The follwing
		subsection explain how to create a Metamorph object, how to
		build a
		processing pipeline and how to handle exceptions.

		\subsubsection{Creation}

		A new Metamorph is created by calling {\tt
		build()} on
		MetamorphBuilder. See listing \ref{creation}. Please note
		that the
		Metamorph object is not thread-safe.

		\begin{lstlisting}[float=htb, label=creation, caption=Creating a
		Metamorph object based on a
		Metamorph description., language=Java]
		final Metamorph metamorph = MetamorphBuilder.build("definition.xml");
		\end{lstlisting}

		\subsubsection{Wiring}
		The Metamorph object acts as a
		pipe element in the data stream. See also
		figure
		\ref{fig:readerwriter}. This means that we must wire it to a
		data
		source (or reader) and a data sink (or writer). Listing
		\ref{codepipeline1} shows how. First all elements of the processing
		chain are created.
		The wiring is done by calling {\tt setReceiver()}.
		The call returns its
		argument, preserving the respective type. Thus the
		calls can be
		chained to build up a pipeline as shown in the listing.
		Finally the
		processing is stared by calling the respective method on
		the data
		source/reader. The method name depends on the reader. In the
		Metamorph
		project {\tt read()} is used by convention.

		\begin{lstlisting}[float=htb, label=codepipeline1,caption= Putting
		together a processing pipeline according to the pattern in section
		\ref{pipeline}., language=Java]
		// create necessary objects
		final
		PicaReader reader = new PicaReader();
		final Metamorph metamorph =
		MetamorphBuilder.build("defnition.xml");
		final ListMapWriter writer =
		new ListMapWriter();

		//wire them
		reader.setReceiver(metamorph).setReceiver(writer);

		//start processing
		reader.read(input);

		\end{lstlisting}

		Listing \ref{codepipeline2} shows a
		few more sophisticated wiring patterns,
		such as adding an additional
		element, junctions or splitters.

		\begin{lstlisting}[float=htb,
		label=codepipeline2,caption= Advanced
		wiring., language=Java]
		//add
		logging
		reader.setReceiver(new
		LogPipe()).setReceiver(metamorph).setReceiver(writer);

		//adding a tee
		junction
		reader.setReceiver(new Tee()).setReceivers(writer1, writer2);

		//splitting based on a metamorph description
		final Splitter splitter =
		new Splitter("morph/typeSplitter.xml");
		reader.setReceiver(splitter).setReceiver("Tn", writer1);
		splitter.setReceiver("Tp", writer2);
		\end{lstlisting}


		\subsubsection{Error Handling}
		If an exception occurs during the
		processing of a stream of records, it
		is back propagated to the first
		element in the chain. This normally
		means that processing is terminated
		which may not be the preferred
		action. Imagine processing a million
		records. One normally prefers to
		log any error but continue the
		processing.
		For this reason an error handler may be registered with the
		Metamorph
		object. It catches all exceptions occurring in the Metamorph
		object
		and below. Listing \ref{errorhandling} shows the respective code
		snippet.
		\begin{lstlisting}[float=htb, label=errorhandling,caption=
		Registering an error handler.,
		language=Java]
		metamorph.setErrorHandler(new MetamorphErrorHandler() {
		@Override
		public void error(final Exception e) {
		// TODO fill in your error
		handling
		code
		}
		});
		\end{lstlisting}


		\subsection{Metamorph Definition
		Language}\label{def_lang}
		The transformation a specific Metamoph
		instance performs are defined in a
		Metamorph definition in XML. See
		also figure \ref{fig:readerwriter}.
		The structure of the XML is
		constrained by a schema ({\tt
		metamorph.xsd}). Listing \ref{morphdef}
		shows the high level
		organization of a Metamorph definition.

		\begin{lstlisting}[float=htb, label=morphdef,caption=Structure of a
		Metamorph definition file.]

		<programlisting language="xml"><![CDATA[
<?xml version="1.0" encoding="UTF-8"?>
		<metamorph xmlns="http://www.culturegraph.org/metamorph"
			xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
			xsi:schemaLocation="http://www.culturegraph.org/metamorph metamorph.xsd"
			entityMarker="." version="2">
			<meta><!-- Metadata -->
			</meta>
			<functions><!-- Function definitions -->
			</functions>
			<rules><!-- Transformation rules -->
			</rules>
			<maps><!-- Data maps -->
			</maps>
		</metamorph>
		]]></programlisting>



		The root element {\tt metamorph} has two attributes:
		One indicates the
		Metamorph version the document it intended to work
		with, the second
		indicates the character used to separate entity names
		(see also
		section
		\ref{dispatching}). Within the {\tt metamorph} tag
		there are
		four
		sections.
		The first and optional one holds metadata for
		the definition
		file. The
		second section -- also optional -- holds
		definition of custom
		functions. See section \ref{customfunc}.
		The {\tt
		rules} block defines
		the actual transformation rules. All of the
		following subsection refer
		to them.
		Finally the optional {\tt maps}
		block allows to define
		maps/dictionaries for
		lookup functionality. See
		section \ref{lookup} for
		details.

	</section>


</chapter>
